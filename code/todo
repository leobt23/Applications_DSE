EDA:

- (Canceled) Normalising the Time amount column. The amount column is not in line with the anonimised features. Time and Amount
- Transactions in time , Features density plot (https://www.kaggle.com/code/gpreda/credit-card-fraud-detection-predictive-models#Data-exploration)
- for wach feature, distributions of fraud vs no fraud (https://www.kaggle.com/code/currie32/predicting-fraud-with-tensorflow)
- (DONE) Converting the time values to hour or day
- (CANCELED) Fit distributions
- (DONE) Fraud rate peak over time (comparing to non fraud. the percentage of each at each time);
- Time to hours of the day (good feature);
- Amount to log of 10 (In feedzai the do that);
- Check hours with more fraud since the dataset is 2 days;
- Can we group into sequences?
- (DONE )Fraud rate over time
- 27 such transactions with 0 euros were a fraudulent transactions 27/492 = 5%
- Outliers (https://www.kaggle.com/code/nareshbhat/outlier-the-silent-killer)
- Dimensionality Reduction and Clustering, t-SNE
- Variation
- ANOVA?
- Apply clock

  pr√©-processing:

- Data clean
- Scale and RobustScale
- Supervised Encoding Methods
- Approaching for Novel Categories
- Spliting w/ - Undersampling, Oversampling , both together or not
- (CANCELED) Correlation after
- (CANCELED) Correlation boxplot
- resampling and simple logistic regression classifier.
- (XXX) Features importance (random forest) (is it only for models?)

- OneClassSVM
- IsolationForest
- Autoencoders
- ensamble models;

Some considerations:
Verify accuracy with and without preprocessing
Never test on the oversampled or undersampled dataset.
If we want to implement cross validation, remember to oversample or undersample your training data during cross-validation, not before!
Don't use accuracy score as a metric with imbalanced datasets (will be usually high and misleading), instead use f1-score, precision/recall score or confusion matrix
